{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrrJ4oNhF0qyN8PO2PM+/x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ASPAditya6/MAE598_HW5/blob/master/MAE598_HW5_P_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "eMuNAPkJf_4F",
        "outputId": "fed59bf4-540f-469b-c01d-ead8e3661f74"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m43\u001b[0m\n\u001b[0;31m    if res.fun < best_acquisition_value:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import sklearn.gaussian_process as gp\n",
        "from scipy.stats import norm\n",
        "from scipy.optimize import minimize\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "def expected_improvement(x, gaussian_process, evaluated_loss, greater_is_better=False, n_params=1):\n",
        "\n",
        "    x_to_predict = x.reshape(-1, n_params)\n",
        "\n",
        "    mu, sigma = gaussian_process.predict(x_to_predict, return_std=True)\n",
        "\n",
        "    if greater_is_better:\n",
        "        loss_optimum = np.max(evaluated_loss)\n",
        "    else:\n",
        "        loss_optimum = np.min(evaluated_loss)\n",
        "\n",
        "    scaling_factor = (-1) ** (not greater_is_better)\n",
        "\n",
        "    with np.errstate(divide='ignore'):\n",
        "        Z = scaling_factor * (mu - loss_optimum) / sigma\n",
        "        expected_improvement = scaling_factor * (mu - loss_optimum) * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
        "        expected_improvement[sigma == 0.0] == 0.0\n",
        "\n",
        "    return -1 * expected_improvement\n",
        "\n",
        "def sample_next_hyperparameter(acquisition_func, gaussian_process, evaluated_loss, greater_is_better=False,\n",
        "                               bounds=(0, 10), n_restarts=25):\n",
        "\n",
        "    best_x = None\n",
        "    best_acquisition_value = 1\n",
        "    n_params = bounds.shape[0]\n",
        "\n",
        "    for starting_point in np.random.uniform(bounds[:, 0], bounds[:, 1], size=(n_restarts, n_params)):\n",
        "\n",
        "         res = minimize(fun=acquisition_func,\n",
        "                       x0=starting_point.reshape(1, -1),\n",
        "                       bounds=bounds,\n",
        "                       method='L-BFGS-B',\n",
        "                       args=(gaussian_process, evaluated_loss, greater_is_better, n_params))\n",
        "\n",
        "       if res.fun < best_acquisition_value:\n",
        "            best_acquisition_value = res.fun\n",
        "            best_x = res.x\n",
        "\n",
        "    return best_x\n",
        "\n",
        "\n",
        "def bayesian_optimisation(n_iters, sample_loss, bounds, x0=None, n_pre_samples=5,\n",
        "                          gp_params=None, random_search=False, alpha=1e-5, epsilon=1e-7):\n",
        "\n",
        "    x_list = []\n",
        "    y_list = []\n",
        "\n",
        "    n_params = bounds.shape[0]\n",
        "\n",
        "    if x0 is None:\n",
        "        for params in np.random.uniform(bounds[:, 0], bounds[:, 1], (n_pre_samples, bounds.shape[0])):\n",
        "            x_list.append(params)\n",
        "            y_list.append(sample_loss(params))\n",
        "    else:\n",
        "        for params in x0:\n",
        "            x_list.append(params)\n",
        "            y_list.append(sample_loss(params))\n",
        "\n",
        "    xp = np.array(x_list)\n",
        "    yp = np.array(y_list)\n",
        "\n",
        "    if gp_params is not None:\n",
        "        model = gp.GaussianProcessRegressor(**gp_params)\n",
        "    else:\n",
        "        kernel = gp.kernels.Matern()\n",
        "        model = gp.GaussianProcessRegressor(kernel=kernel,\n",
        "                                            alpha=alpha,\n",
        "                                            n_restarts_optimizer=10,\n",
        "                                            normalize_y=True)\n",
        "\n",
        "    for n in range(n_iters):\n",
        "\n",
        "        model.fit(xp, yp)\n",
        "\n",
        "        if random_search:\n",
        "            x_random = np.random.uniform(bounds[:, 0], bounds[:, 1], size=(random_search, n_params))\n",
        "            ei = -1 * expected_improvement(x_random, model, yp, greater_is_better=True, n_params=n_params)\n",
        "            next_sample = x_random[np.argmax(ei), :]\n",
        "        else:\n",
        "            next_sample = sample_next_hyperparameter(expected_improvement, model, yp, greater_is_better=True, bounds=bounds, n_restarts=100)\n",
        "\n",
        "        if np.any(np.abs(next_sample - xp) <= epsilon):\n",
        "            next_sample = np.random.uniform(bounds[:, 0], bounds[:, 1], bounds.shape[0])\n",
        "\n",
        "        cv_score = sample_loss(next_sample)\n",
        "\n",
        "        x_list.append(next_sample)\n",
        "        y_list.append(cv_score)\n",
        "\n",
        "        xp = np.array(x_list)\n",
        "        yp = np.array(y_list)\n",
        "\n",
        "    return xp, yp\n",
        "def Obj(X):\n",
        "    f = (4 - 2.1 * X[0] ** 2 + X[0] ** 4 / 3) * X[0] ** 2 + X[0] * X[1] + (-4 + 4 * X[1] ** 2) * X[1] ** 2\n",
        "    return f\n",
        "\n",
        "n_iters = 100\n",
        "# X0 = np.array([0, 0])\n",
        "bounds = np.array([[-3, 3], [-2, 2]])\n",
        "\n",
        "xp, yp = bayesian_optimisation(n_iters, Obj, bounds, x0=None, n_pre_samples=5, gp_params=None, random_search=False, alpha=1e-5, epsilon=1e-7)\n",
        "\n",
        "index = int(np.where(yp == min(yp))[0])\n",
        "\n",
        "print('Optimal function value found in '+str(index)+'th iteration')\n",
        "print('Optimal x1 within', n_iters, 'iterations:',xp[index,0], '\\n', 'Optimal x2 within', n_iters, 'iterations:',xp[index,1])\n",
        "print('Optimized Minimum Function Value:', yp[index])\n",
        "\n",
        "# # Making sure this is doing what I think it is by testing a solution:\n",
        "# X = [-0.11828430500892839, 0.6953188401727513]\n",
        "# y = Obj(X)\n",
        "# print(y) # should be -1.0255975771369195, it is!"
      ]
    }
  ]
}